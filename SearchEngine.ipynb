{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *STEP1:* 将原来6G的xml文件切分成平均1M的小文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RUN = 0\n",
    "if(RUN):  \n",
    "    fin = open('zhwiki-20171020-pages-articles-multistream.xml',encoding='utf8')\n",
    "    for i in range(10000000):      \n",
    "        readin = fin.readlines(1000*1000)\n",
    "        if(not readin):\n",
    "            break\n",
    "        fout = open('parts/part_'+str(i),'w',encoding = 'utf8')\n",
    "        fout.writelines(readin)\n",
    "        # fout.writelines(re.sub(\"[\\[\\]\\'\\\"\\@#$%^&*()<>：{}|/_.+-—，,。!！ ？、~@#￥%……&*（）]+\", \"\",s))\n",
    "        fout.close()\n",
    "    fin.close()\n",
    "    partsNum = i\n",
    "    print(partsNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION** 判断一个unicode是否为汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_ch(uchar):\n",
    "        \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "        if uchar >= u'\\u4e00' and uchar<=u'\\u9fa5':\n",
    "                return True\n",
    "        else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *STEP2:* 导入GBK编码表，构造一个dic，将汉字映射到整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典大小： 20902\n"
     ]
    }
   ],
   "source": [
    "with open('GBK',encoding='utf8') as fin:\n",
    "    chList = []\n",
    "    for line in fin.readlines():\n",
    "        for word in line:\n",
    "            if(is_ch(word)):\n",
    "                chList.append(word)\n",
    "\n",
    "                \n",
    "chNum = len(chList)\n",
    "print('词典大小：',chNum)\n",
    "ch2int = {}\n",
    "idx = 0\n",
    "with open('ch2int','w',encoding='utf8') as fout:  \n",
    "    for w in chList:\n",
    "        ch2int[w] = idx\n",
    "        fout.write(w+'\\t'+str(idx)+'\\n')\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASS** 定义类BitSet 用于集合运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BitSet:\n",
    "    def __init__(self, elementNum):\n",
    "        self.bytesNum = round(elementNum/8)\n",
    "        self.set = bytearray(self.bytesNum)\n",
    "        \n",
    "    def insert(self, index):\n",
    "        if(index >= 0 and index>>3 < self.bytesNum):\n",
    "            self.set[index >> 3] = self.set[index >> 3] | (1 << int(index & 7)) \n",
    "            return True;\n",
    "        return False;\n",
    "    \n",
    "    def isElement(self, index):\n",
    "        if(index >= 0 and index>>3 < self.bytesNum and (self.set[index >> 3] & (1<<(index & 7)))):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def AND(self, bs):\n",
    "        a = BitSet(self.bytesNum*8)\n",
    "        for i in range(self.bytesNum):\n",
    "            a.set[i] = self.set[i] & bs.set[i]\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *STEP3:* 给每一个汉字创建倒排索引，对应包含该汉字的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个字的倒排索引表的字节数： 615\n"
     ]
    }
   ],
   "source": [
    "partsNum = 4919\n",
    "bytesNum = round(partsNum/8)\n",
    "\n",
    "invertedIndex = [BitSet(partsNum) for i in range(chNum)]\n",
    "print('每个字的倒排索引表的字节数：',invertedIndex[0].bytesNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION** 显示进度条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,time\n",
    "\n",
    "def processBar(index, totalNum):\n",
    "    bar = '#'*int((index/totalNum)*50)\n",
    "    sys.stdout.write(str(int((index/totalNum)*100))+'%  ||'+bar+'->'+\"\\r\")\n",
    "    #sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遍历所有文档的所有字符，建立倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN_iidx = 0\n",
    "if(RUN_iidx):\n",
    "    for partIdx in range(partsNum):\n",
    "\n",
    "        processBar(partIdx, partsNum)\n",
    "\n",
    "        with open('parts/part_'+str(partIdx),encoding='utf8') as f:\n",
    "            for word in f.read():\n",
    "                try:\n",
    "                    if(is_ch(word)):\n",
    "                        invertedIndex[ch2int[word]].insert(partIdx)\n",
    "                except KeyError as err:\n",
    "                    print('KEY ERROR'+str(err))\n",
    "\n",
    "    for chIdx in range(chNum):    \n",
    "        processBar(chIdx, chNum)\n",
    "        with open('iidx/ch_'+str(chIdx)+'.iidx','w',encoding='utf8') as f:\n",
    "                for j in invertedIndex[chIdx].set:\n",
    "                    f.write(str(j)+' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试：读取倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor chIdx in range(chNum):\\n    processBar(chIdx,chNum)\\n    f = 'iidx/ch_'+str(chIdx)+'.iidx'\\n    invertedIndex[chIdx] = readIidx(f)\\n    \\nprint(invertedIndex[0].set[:20])\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readIidx(file):\n",
    "    with open(file,encoding='utf8') as f:\n",
    "        s = f.read()[:-1].split(' ')\n",
    "    bSet = BitSet(len(s)*8)\n",
    "    for i in range(bSet.bytesNum):\n",
    "        bSet.set[i] = int(s[i])\n",
    "    return bSet\n",
    "\n",
    "\"\"\"\n",
    "for chIdx in range(chNum):\n",
    "    processBar(chIdx,chNum)\n",
    "    f = 'iidx/ch_'+str(chIdx)+'.iidx'\n",
    "    invertedIndex[chIdx] = readIidx(f)\n",
    "    \n",
    "print(invertedIndex[0].set[:20])\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *STEP4:* 建立每个文档的 tf 表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%  ||#################################################->\r"
     ]
    }
   ],
   "source": [
    "# tf \n",
    "RUN_tf = 1\n",
    "\n",
    "if(RUN_tf):\n",
    "    for partIdx in range(partsNum):\n",
    "\n",
    "        tf = [0.0 for i in range(chNum)]\n",
    "        total = 0\n",
    "\n",
    "        with open('parts/part_'+str(partIdx),encoding='utf8') as f:\n",
    "            for word in f.read():\n",
    "                if(is_ch(word)):\n",
    "                    tf[ch2int[word]] += 1\n",
    "                    total += 1\n",
    "\n",
    "        total = max(total, 1)\n",
    "        for i in range(chNum):\n",
    "            tf[i] /= total\n",
    "\n",
    "        with open('parts/tf/part_'+str(partIdx)+'.tf','w',encoding='utf8') as fout:\n",
    "            for i in tf:\n",
    "                fout.write(str(i)+' ')\n",
    "                \n",
    "        processBar(partIdx, partsNum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### *STEP5:* 计算每个单词的 idf 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%  ||#################################################->\r"
     ]
    }
   ],
   "source": [
    "# idf(ch) = log(total DocumentNum / Num of documents containing ch)\n",
    "import math\n",
    "\n",
    "RUN_idf = 1\n",
    "if(RUN_idf):\n",
    "    \n",
    "    idf = [0.0 for i in range(chNum)]\n",
    "\n",
    "    for chIdx in range(chNum):\n",
    "        processBar(chIdx, chNum)\n",
    "        num = 0\n",
    "        iidxFile = 'iidx/ch_'+str(chIdx)+'.iidx'\n",
    "        bs = readIidx(iidxFile)\n",
    "        for i in range(partsNum):\n",
    "            if(bs.isElement(i)):\n",
    "                num += 1\n",
    "        idf[chIdx] = math.log(partsNum/(num+1)) # divide by zero\n",
    "\n",
    "    with open('iidx/idf','w',encoding='utf8') as fout:\n",
    "        for i in idf:\n",
    "            fout.write(str(i)+' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取每篇文档每个字的 tfidf 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%  ||#################################################->\r"
     ]
    }
   ],
   "source": [
    "idfFile = 'iidx/idf'\n",
    "with open(idfFile, encoding='utf8') as f:\n",
    "    s = f.read().split()\n",
    "IDF = [float(idf) for idf in s]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ENOUGHMEMORY = True\n",
    "if(ENOUGHMEMORY):\n",
    "    TFIDF = np.zeros([partsNum,chNum])\n",
    "    for partIdx in range(partsNum):\n",
    "        processBar(partIdx, partsNum)\n",
    "        file = 'parts/tf/part_'+str(partIdx)+'.tf'\n",
    "        with open(file,encoding='utf8') as f:\n",
    "            s = f.read().split()    \n",
    "        for chIdx in range(chNum):\n",
    "            TFIDF[partIdx][chIdx] = float(s[chIdx])*IDF[chIdx]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *STEP6:* 搜索到包含关键词的文档集, 将搜索到的文档集根据 tf-idf 值排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask(bytes):\n",
    "    r = BitSet(bytes*8)\n",
    "    for i in range(bytes):\n",
    "        r.set[i] = 255\n",
    "    return r\n",
    "\n",
    "bsMask = mask(bytesNum) # 掩码\n",
    "\n",
    "def tfidf(partIdx, keyList):\n",
    "    \n",
    "    if(ENOUGHMEMORY):\n",
    "        return  sum([TFIDF[partIdx][chIdx] for chIdx in keyList])\n",
    "    \n",
    "    tfidf = 0\n",
    "    file = 'parts/tf/part_'+str(partIdx)+'.tf'\n",
    "    with open(file,encoding='utf8') as f:\n",
    "        s = f.read().split()\n",
    "        for i in keyList:\n",
    "            tfidf += float(s[i])*IDF(i) \n",
    "    return tfidf\n",
    "\n",
    "def search(query):\n",
    "    keyList = []\n",
    "    targets = bsMask\n",
    "    for word in query:\n",
    "        if(is_ch(word)):\n",
    "            keyList.append(ch2int[word])\n",
    "            targets = targets.AND(readIidx('iidx/ch_'+str(ch2int[word])+'.iidx'))\n",
    "            \n",
    "    result = []\n",
    "    for i in range(partsNum):\n",
    "        if(targets.isElement(i)):\n",
    "            result.append(i)\n",
    "\n",
    "    # rearrange\n",
    "    tuples = []\n",
    "    for doc in result:\n",
    "        tuples.append((doc, tfidf(doc,keyList)))\n",
    "    \n",
    "    result = sorted(tuples, key = lambda x:x[1], reverse=1)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（文档号，TFIDF值）\n",
      "(2195, 0.0012538194937865626)\n",
      "(2192, 0.00074516821934332783)\n",
      "(3335, 0.00070944251423168069)\n",
      "(3334, 0.00053639815331373464)\n",
      "(2191, 0.00048238345187170538)\n",
      "(2554, 0.00040487345298292432)\n",
      "(3336, 0.00040323815270376802)\n",
      "(4891, 0.00040130384277274974)\n",
      "(1225, 0.00037096059308915096)\n",
      "(4021, 0.0003702145265853077)\n",
      "(1746, 0.00036683432065730661)\n",
      "(2698, 0.00036536644465616285)\n",
      "(2541, 0.00036071489111920478)\n",
      "(2280, 0.00035553127974912075)\n",
      "(2279, 0.00034865903782981938)\n",
      "(2546, 0.00034503278985721354)\n",
      "(3508, 0.00034479396238077525)\n",
      "(2196, 0.00034082480195172452)\n",
      "(3254, 0.00032731985297806482)\n",
      "(2983, 0.00031677609220833982)\n",
      "(336, 0.0003128326008106595)\n",
      "(3258, 0.00031002191307890589)\n",
      "(3327, 0.00030376847836756685)\n",
      "(4906, 0.00029932409207689028)\n",
      "(3903, 0.00029883267483686002)\n",
      "(2199, 0.00028384795608407014)\n",
      "(1945, 0.00028228236669312597)\n",
      "(4904, 0.00028202085431423379)\n",
      "(1608, 0.00027640804007445993)\n",
      "(4900, 0.00027293930198780904)\n",
      "(4869, 0.00027201629958735067)\n",
      "(1394, 0.00026877560725819047)\n",
      "(3246, 0.00026631756797022556)\n",
      "(2987, 0.00026519494047229413)\n",
      "(4907, 0.00026432498054429374)\n",
      "(52, 0.00026301230839225734)\n",
      "(216, 0.00025874007463873105)\n",
      "(3328, 0.00025796200894640415)\n",
      "(791, 0.00025724854539680965)\n",
      "(4090, 0.00025699421469040296)\n",
      "(535, 0.00025609537549578135)\n",
      "(3065, 0.0002554011581714378)\n",
      "(3260, 0.00025449326564358005)\n",
      "(1825, 0.00025202601683480452)\n",
      "(4476, 0.00025075326567241044)\n",
      "(4888, 0.00025044227653619506)\n",
      "(4898, 0.00024722556470796692)\n",
      "(3852, 0.00024633644713285043)\n",
      "(1588, 0.00024600612726349396)\n",
      "(4856, 0.00024517234771474853)\n",
      "(534, 0.00024504396701553234)\n",
      "(4868, 0.00024483335812760815)\n",
      "(1101, 0.00024164843021263636)\n",
      "(2870, 0.00024121001051787618)\n",
      "(3929, 0.00024095305937715602)\n",
      "(1219, 0.0002365264026054875)\n",
      "(1642, 0.00023110225996569246)\n",
      "(4908, 0.00022699347070931361)\n",
      "(2007, 0.00022316506216776643)\n",
      "(3255, 0.00022309364689232166)\n",
      "(4475, 0.00022299100423076046)\n",
      "(2989, 0.00022148578284090421)\n",
      "(566, 0.00022080822227676236)\n",
      "(284, 0.00021781605983445165)\n",
      "(578, 0.00021739149247629755)\n",
      "(2647, 0.00021705046646544355)\n",
      "(53, 0.00021608884740416864)\n",
      "(703, 0.00021559052009534463)\n",
      "(4870, 0.00021276261809173791)\n",
      "(1996, 0.00021243928592778249)\n",
      "(869, 0.00021205156469604477)\n",
      "(3313, 0.00021102414589577517)\n",
      "(2277, 0.00021027129971151214)\n",
      "(764, 0.00020991022188945066)\n",
      "(4078, 0.00020924333229085069)\n",
      "(1411, 0.00020779014350972414)\n",
      "(1665, 0.00020548149425124969)\n",
      "(516, 0.0002050535023059429)\n",
      "(3507, 0.00020478907262873832)\n",
      "(3257, 0.0002028913469307221)\n",
      "(2682, 0.00020238497918519523)\n",
      "(1217, 0.00020221374975497555)\n",
      "(2875, 0.0002021241575849941)\n",
      "(1138, 0.0002019831118262905)\n",
      "(2608, 0.00020142277879299069)\n",
      "(4126, 0.00020121234693387812)\n",
      "(1546, 0.00020024372418735506)\n",
      "(2773, 0.00019754264582015784)\n",
      "(1238, 0.00019716172771967383)\n",
      "(1652, 0.00019709667547514564)\n",
      "(3047, 0.00019665719523411551)\n",
      "(1304, 0.00019661670234938968)\n",
      "(54, 0.00019635568703657609)\n",
      "(2874, 0.00019603567500628286)\n",
      "(4770, 0.00019388248990055321)\n",
      "(4795, 0.00019368130092328959)\n",
      "(2339, 0.0001936803544847363)\n",
      "(1561, 0.00019318877902324512)\n",
      "(637, 0.00019307122103405256)\n",
      "(3944, 0.00019228010990341406)\n"
     ]
    }
   ],
   "source": [
    "r = search('降龙十八掌')[:100]\n",
    "print(\"（文档号，TFIDF值）\")\n",
    "for i in r:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如每次只取出前十名，每篇文档大小1M，总共只占用10M内存，完全可以将所有文字体取出来分析，找到最佳匹配的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4cc1a4c4cd50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'降龙十八掌'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdocList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdocIdx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'parts/part_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocIdx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "query = '降龙十八掌'\n",
    "docList = [item[0] for item in search(query)[:100]]\n",
    "s = []\n",
    "for docIdx in docList:\n",
    "    with open('parts/part_'+str(docIdx),encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            check = True # 只有当所有关键词都在一句话中时为 True\n",
    "            for word in query:\n",
    "                if(word not in line):\n",
    "                    check = False\n",
    "                    break\n",
    "            if(check):\n",
    "                s.append('part_'+str(docIdx)+'   '+line)\n",
    "        \n",
    "print('前百篇文档的有效行数:',len(s))\n",
    "print('包含所有关键词的句段如下：')\n",
    "s[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "测试：繁简转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langconv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-4370d0d27895>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlangconv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'為当今世界著名的[[美國]][[網球]]女子巨星，網球史上最伟大的女子选手之一，    美國體育史上最佳女子網球选手（與[[克里斯·埃弗特|Chris Evert]]並列），    全世界第一位網球大滿貫黑人女單冠軍，單打最高世界排名第一，    23座大滿貫女單冠軍得主，網球史上第二位女子單打生涯全滿貫得主。'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 转换繁体到简体\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'zh-hans'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langconv'"
     ]
    }
   ],
   "source": [
    "from langconv import *\n",
    "\n",
    "line = '為当今世界著名的[[美國]][[網球]]女子巨星，網球史上最伟大的女子选手之一，\\\n",
    "    美國體育史上最佳女子網球选手（與[[克里斯·埃弗特|Chris Evert]]並列），\\\n",
    "    全世界第一位網球大滿貫黑人女單冠軍，單打最高世界排名第一，\\\n",
    "    23座大滿貫女單冠軍得主，網球史上第二位女子單打生涯全滿貫得主。'\n",
    "# 转换繁体到简体\n",
    "line = Converter('zh-hans').convert(line)\n",
    "print(line)\n",
    "# 转换简体到繁体\n",
    "line = Converter('zh-hant').convert(line)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
